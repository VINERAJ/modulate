{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ef57414b6d4c76b5af61bfb1838e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mWav2Vec2ForSequenceClassification LOAD REPORT\u001b[0m from: ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\n",
      "Key                      | Status     | \n",
      "-------------------------+------------+-\n",
      "classifier.output.bias   | UNEXPECTED | \n",
      "classifier.output.weight | UNEXPECTED | \n",
      "classifier.dense.weight  | UNEXPECTED | \n",
      "classifier.dense.bias    | UNEXPECTED | \n",
      "projector.bias           | MISSING    | \n",
      "classifier.bias          | MISSING    | \n",
      "classifier.weight        | MISSING    | \n",
      "projector.weight         | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForAudioClassification, Wav2Vec2FeatureExtractor\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "\n",
    "# Load the model and extractor from your notebook's logic\n",
    "model_name = \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\"\n",
    "model = AutoModelForAudioClassification.from_pretrained(model_name)\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "\n",
    "# Emotion mapping\n",
    "id2label = {\n",
    "    \"0\": \"angry\", \"1\": \"calm\", \"2\": \"disgust\", \"3\": \"fearful\",\n",
    "    \"4\": \"happy\", \"5\": \"neutral\", \"6\": \"sad\", \"7\": \"surprised\"\n",
    "}\n",
    "\n",
    "# Define the output filename simply as a string\n",
    "OUTPUT_FILE = \"audio.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {\n",
    "    '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
    "    '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
    "} # map to emotions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mood_result(audio_file):\n",
    "    # Process audio at 16000Hz as the model expects\n",
    "    sound = AudioSegment.from_file(audio_file)\n",
    "    sound = sound.set_frame_rate(16000)\n",
    "    sound_array = np.array(sound.get_array_of_samples())\n",
    "\n",
    "    input_values = feature_extractor(\n",
    "        raw_speech=sound_array,\n",
    "        sampling_rate=16000,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values.input_values.float()).logits\n",
    "        \n",
    "    predicted_id = torch.argmax(logits, dim=-1).item()\n",
    "    return id2label[str(predicted_id)] # highest prob. emotion\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mel spectogram converts audio to visual form\n",
    "# # uses librosa library\n",
    "\n",
    "# def extract_features(audio_data):\n",
    "  \n",
    "#        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast', duration=5, sr=22050)\n",
    "        \n",
    "#        mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "#         # Mel-frequency cepstral coefficients - power spectrum of sound\n",
    "#        chroma = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate).T, axis=0)\n",
    "#         # chromagram -> vector representation of audio, representing pitch on chromatic scale\n",
    "#        mel = np.mean(librosa.feature.melspectrogram(y=audio, sr=sample_rate).T, axis=0)\n",
    "#         # compute mel-scaled spectrogram\n",
    "        \n",
    "#        features = np.hstack([mfccs, chroma, mel])\n",
    "#        return features  \n",
    "\n",
    "\n",
    "\n",
    " \n",
    " \n",
    " \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "\n",
    "# MODEL_PATH = 'ravdess-pretrained.ipynb' \n",
    "# try:\n",
    "#     model = joblib.load(MODEL_PATH)\n",
    "#     print(\"Model loaded successfully!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading model: {e}. Check the filename!\")\n",
    "# emotions = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording... Press Enter to stop.\n",
      "Recording saved. Analyzing...\n",
      "You said: \"okay\"\n",
      "Mood: CALM ---\n",
      "Recommended song (random_from_songs_csv): I Love Kanye - Kanye West\n",
      "Spotify: https://open.spotify.com/track/4S8d14HvHb70ImctNgVzQQ\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import speech_recognition as sr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForAudioClassification, Wav2Vec2FeatureExtractor\n",
    "from pydub import AudioSegment\n",
    "\n",
    "songs_df = pd.read_csv(\"list.csv\")\n",
    "\n",
    "def canonical_mood(label):\n",
    "    label = str(label).strip().lower()\n",
    "    mood_aliases = {\n",
    "        \"anger\": \"angry\",\n",
    "        \"angry\": \"angry\",\n",
    "        \"calm\": \"calm\",\n",
    "        \"disgust\": \"disgust\",\n",
    "        \"fear\": \"fearful\",\n",
    "        \"fearful\": \"fearful\",\n",
    "        \"happy\": \"happy\",\n",
    "        \"happiness\": \"happy\",\n",
    "        \"neutral\": \"neutral\",\n",
    "        \"sad\": \"sad\",\n",
    "        \"sadness\": \"sad\",\n",
    "        \"surprise\": \"surprised\",\n",
    "        \"surprised\": \"surprised\"\n",
    "    }\n",
    "    return mood_aliases.get(label, label)\n",
    "\n",
    "def ensure_mood_model_loaded():\n",
    "    global model, feature_extractor, predict_mood_result\n",
    "    \n",
    "    if globals().get(\"_mood_model_version\") == 2 and \"predict_mood_result\" in globals():\n",
    "        return\n",
    "    \n",
    "    model_name = \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\"\n",
    "    model = AutoModelForAudioClassification.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "\n",
    "    id2label_cfg = model.config.id2label\n",
    "\n",
    "    def predict_mood_result(audio_file):\n",
    "        # Resample and convert to mono\n",
    "        sound = AudioSegment.from_file(audio_file)\n",
    "        sound = sound.set_frame_rate(16000).set_channels(1)\n",
    "        samples = np.array(sound.get_array_of_samples(), dtype=np.float32)\n",
    "        \n",
    "        # Normalize waveform to [-1, 1]\n",
    "        max_abs = np.max(np.abs(samples))\n",
    "        if max_abs > 0:\n",
    "            samples = samples / max_abs\n",
    "\n",
    "        inputs = feature_extractor(\n",
    "            raw_speech=samples,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "\n",
    "        predicted_id = int(torch.argmax(logits, dim=-1).item())\n",
    "        raw_label = id2label_cfg.get(predicted_id, str(predicted_id))\n",
    "        return canonical_mood(raw_label)\n",
    "\n",
    "    globals()[\"predict_mood_result\"] = predict_mood_result\n",
    "    globals()[\"_mood_model_version\"] = 2\n",
    "\n",
    "def get_song_for_mood(mood):\n",
    "    mood = mood.strip().lower()\n",
    "\n",
    "    # Try exact mood match first\n",
    "    mood_matches = songs_df[\n",
    "        songs_df[\"labels\"].fillna(\"\").str.lower().str.contains(fr\"\\\\b{mood}\\\\b\", regex=True)\n",
    "    ]\n",
    "    if not mood_matches.empty:\n",
    "        song = mood_matches.sample(1).iloc[0]\n",
    "        return {\n",
    "            \"artist\": song[\"artist\"],\n",
    "            \"title\": song[\"title\"],\n",
    "            \"spotify_url\": song[\"spotify_url\"],\n",
    "            \"source\": \"mood_match\"\n",
    "        }\n",
    "\n",
    "    # If no mood match in songs.csv, return a random song from songs.csv (no fallback list)\n",
    "    random_song = songs_df.sample(1).iloc[0]\n",
    "    return {\n",
    "        \"artist\": random_song[\"artist\"],\n",
    "        \"title\": random_song[\"title\"],\n",
    "        \"spotify_url\": random_song[\"spotify_url\"],\n",
    "        \"source\": \"random_from_songs_csv\"\n",
    "    }\n",
    "\n",
    "def record_audio_until_enter(output_file=\"current_voice.wav\", fs=44100):\n",
    "    print(\"Recording... Press Enter to stop.\")\n",
    "    chunks = []\n",
    "\n",
    "    def callback(indata, frames, time_info, status):\n",
    "        if status:\n",
    "            print(status)\n",
    "        chunks.append(indata.copy())\n",
    "\n",
    "    with sd.InputStream(samplerate=fs, channels=1, dtype=\"float32\", callback=callback):\n",
    "        input()  # stop recording when user presses Enter\n",
    "\n",
    "    if not chunks:\n",
    "        raise RuntimeError(\"No audio captured. Please try again.\")\n",
    "\n",
    "    recording = np.concatenate(chunks, axis=0)\n",
    "    recording_int16 = (recording * 32767).astype(np.int16)\n",
    "    write(output_file, fs, recording_int16)\n",
    "    return output_file\n",
    "\n",
    "def record_and_get_mood():\n",
    "    ensure_mood_model_loaded()\n",
    "\n",
    "    output_file = record_audio_until_enter()\n",
    "    print(\"Recording saved. Analyzing...\")\n",
    "\n",
    "    # Speech to Text\n",
    "    r = sr.Recognizer()\n",
    "    try:\n",
    "        with sr.AudioFile(output_file) as source:\n",
    "            audio_data = r.record(source)\n",
    "            text = r.recognize_google(audio_data)\n",
    "            print(f\"You said: \\\"{text}\\\"\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not transcribe speech: {e}\")\n",
    "\n",
    "    # Emotion + Song Recommendation\n",
    "    try:\n",
    "        mood = predict_mood_result(output_file)\n",
    "        print(f\"Mood: {mood.upper()} ---\")\n",
    "\n",
    "        song = get_song_for_mood(mood)\n",
    "        print(f\"Recommended song ({song['source']}): {song['title']} - {song['artist']}\")\n",
    "        print(f\"Spotify: {song['spotify_url']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Emotion analysis or recommendation failed: {e}\")\n",
    "\n",
    "# Run the process\n",
    "record_and_get_mood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
